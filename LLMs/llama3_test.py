'''
model_dir = "./llama3/llama-3-8b-Instruct-hf"
model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        device_map="auto",
    )
tokenizer = AutoTokenizer.from_pretrained(model_dir)

pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.float16,
    device_map="auto",
)

sequences = pipeline(
    'I have tomatoes, basil and cheese at home. What can I cook for dinner?\n',
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=400,
)
for seq in sequences:
    print(f"{seq['generated_text']}")
'''

import torch
import transformers
import time
from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM

model_dir = "./llama3/Llama-3-8B-Instruct-hf"
model_name=model_dir.split("/")[-1]

model = AutoModelForCausalLM.from_pretrained(model_dir,device_map="auto")
#model = LlamaForCausalLM.from_pretrained(model_dir)
#tokenizer = LlamaTokenizer.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

pipeline = transformers.pipeline("text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        device_map="auto")

def gen_response(prompt):
    sequences = pipeline(
            prompt,
            do_sample=True,
            top_k=10,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            max_length=1000)
    return sequences

def conti_conv():
    while True:
        prompt=input("Enter prompt: ")
        stime=time.time()
        sequences=gen_response(prompt+"\n")
        for seq in sequences:
            print(f"Response: {seq['generated_text']}")
        print(f"This output was generated by {model_name} and time elapsed is {time.time() - stime:.2f}s. \n")

conti_conv()
